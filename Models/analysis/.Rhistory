}
stops$file_id <- NULL
stops <- unique(stops)
# ! commented: finds duplicates although None: ??? - issue with different version of dplyr? - tidyverse is a bloody mess
#if (any(duplicated(stops$stop_id))) {
#  stop("Duplicated Stop IDS")
#}
if (any(duplicated(routes$route_id))) {
message("De-duplicating route_id")
route_id <- routes[, c("file_id", "route_id")]
if (any(duplicated(route_id))) {
if (force) {
routes <- routes[!duplicated(route_id), ]
}
else {
stop("Duplicated route_id within the same GTFS file, try using force = TRUE")
}
}
route_id$route_id_new <- seq(1, nrow(route_id))
routes <- dplyr::left_join(routes, route_id, by = c("file_id",
"route_id"))
routes <- routes[, c("route_id_new", "agency_id", "route_short_name",
"route_long_name", "route_desc", "route_type")]
names(routes) <- c("route_id", "agency_id", "route_short_name",
"route_long_name", "route_desc", "route_type")
}
if (any(duplicated(calendar$service_id))) {
message("De-duplicating service_id")
service_id <- calendar[, c("file_id", "service_id")]
if (any(duplicated(service_id))) {
stop("Duplicated service_id within the same GTFS file")
}
service_id$service_id_new <- seq(1, nrow(service_id))
calendar <- dplyr::left_join(calendar, service_id, by = c("file_id",
"service_id"))
calendar <- calendar[, c("service_id_new", "monday",
"tuesday", "wednesday", "thursday", "friday", "saturday",
"sunday", "start_date", "end_date")]
names(calendar) <- c("service_id", "monday", "tuesday",
"wednesday", "thursday", "friday", "saturday", "sunday",
"start_date", "end_date")
if (nrow(calendar_dates) > 0) {
calendar_dates <- dplyr::left_join(calendar_dates,
service_id, by = c("file_id", "service_id"))
calendar_dates <- calendar_dates[, c("service_id_new",
"date", "exception_type")]
names(calendar_dates) <- c("service_id", "date",
"exception_type")
}
}
if (any(duplicated(trips$trip_id))) {
message("De-duplicating trip_id")
trip_id <- trips[, c("file_id", "trip_id")]
if (any(duplicated(trip_id))) {
stop("Duplicated trip_id within the same GTFS file")
}
trip_id$trip_id_new <- seq(1, nrow(trip_id))
trips <- dplyr::left_join(trips, trip_id, by = c("file_id",
"trip_id"))
trips <- trips[, c("route_id", "service_id", "trip_id_new",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
stop_times <- dplyr::left_join(stop_times, trip_id, by = c("file_id",
"trip_id"))
stop_times <- stop_times[, c("trip_id_new", "arrival_time",
"departure_time", "stop_id", "stop_sequence", "timepoint")]
names(stop_times) <- c("trip_id", "arrival_time", "departure_time",
"stop_id", "stop_sequence", "timepoint")
}
if (exists("service_id")) {
trips <- dplyr::left_join(trips, service_id, by = c("file_id",
"service_id"))
trips <- trips[, c("route_id", "service_id_new", "trip_id",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
}
if (exists("route_id")) {
trips <- dplyr::left_join(trips, route_id, by = c("file_id",
"route_id"))
trips <- trips[, c("route_id_new", "service_id", "trip_id",
"file_id")]
names(trips) <- c("route_id", "service_id", "trip_id",
"file_id")
}
trips <- trips[, c("route_id", "service_id", "trip_id")]
names(trips) <- c("route_id", "service_id", "trip_id")
if (nrow(calendar_dates) > 0) {
message("Condensing duplicated service patterns")
calendar_dates_summary <- dplyr::group_by(calendar_dates,
service_id)
if (class(calendar_dates_summary$date) == "Date") {
calendar_dates_summary <- dplyr::summarise(calendar_dates_summary,
pattern = paste(c(as.character(date), exception_type),
collapse = ""))
}
else {
calendar_dates_summary <- dplyr::summarise(calendar_dates_summary,
pattern = paste(c(date, exception_type), collapse = ""))
}
calendar_summary <- dplyr::left_join(calendar, calendar_dates_summary,
by = "service_id")
calendar_summary <- dplyr::group_by(calendar_summary,
start_date, end_date, monday, tuesday, wednesday,
thursday, friday, saturday, sunday, pattern)
calendar_summary$service_id_new <- dplyr::group_indices(calendar_summary)
calendar_summary <- calendar_summary[, c("service_id_new",
"service_id")]
trips <- dplyr::left_join(trips, calendar_summary, by = c("service_id"))
trips <- trips[, c("route_id", "service_id_new", "trip_id")]
names(trips) <- c("route_id", "service_id", "trip_id")
calendar <- dplyr::left_join(calendar, calendar_summary,
by = c("service_id"))
calendar <- calendar[, c("service_id_new", "monday",
"tuesday", "wednesday", "thursday", "friday", "saturday",
"sunday", "start_date", "end_date")]
names(calendar) <- c("service_id", "monday", "tuesday",
"wednesday", "thursday", "friday", "saturday", "sunday",
"start_date", "end_date")
calendar <- calendar[!duplicated(calendar$service_id),
]
calendar_dates <- dplyr::left_join(calendar_dates, calendar_summary,
by = c("service_id"))
calendar_dates <- calendar_dates[, c("service_id_new",
"date", "exception_type")]
names(calendar_dates) <- c("service_id", "date", "exception_type")
calendar_dates <- calendar_dates[!duplicated(calendar_dates$service_id),
]
}
stop_times$file_id <- NULL
routes$file_id <- NULL
calendar$file_id <- NULL
res_final <- list(agency, stops, routes, trips, stop_times,
calendar, calendar_dates)
names(res_final) <- c("agency", "stops", "routes", "trips",
"stop_times", "calendar", "calendar_dates")
return(res_final)
}
stopids = list()
for(i in 1:2){#1:length(datalist)){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
# Error duplicated agencies Duplicated Agency IDs 1 3 DC 2 7778462 TEL CTG RRS RBUS 4 A2BR ARBB CX EB GLAR HCC MN MT O2 SK SV WCT ATL CAR DGC JOH THC
# -> force = T
res = gtfs_merge_force(cleandata, force=T)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
res = gtfs_merge_force(cleandata, force=T)
res
merge_all_gtfs<-function(datadir,region_codes = c('EA','EM','L','NE','NW','S','SE','SW','W','WM'),files=paste0(region_codes,'_gtfs.zip')){
datalist = lapply(files,function(f){return(gtfs_read(paste0(datadir,f)))})
names(datalist) <-region_codes
cleandata = datalist
# some cleaning to do: gtfs_merge has bugs
# names(datalist[[1]])
# sapply(datalist[[1]],names)
# Column `stop_id` can't be converted from character to numeric
# datalist[[1]][["stops"]][["stop_id"]] # -> numeric ids !: use hash?
# sapply(md5(datalist[[1]][["stops"]][["stop_id"]]),hex_to_int)
#  + store new ids in a HashSet for stop times
# ! duplicated stop ids -> force does not work: concatenate region code
stopids = list()
for(i in 1:length(datalist)){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
# Error duplicated agencies Duplicated Agency IDs 1 3 DC 2 7778462 TEL CTG RRS RBUS 4 A2BR ARBB CX EB GLAR HCC MN MT O2 SK SV WCT ATL CAR DGC JOH THC
# -> force = T
res = gtfs_merge_force(cleandata, force=T)
gtfs_write(res,folder = datadir,name='all_gtfs')
}
targetdir
merge_all_gtfs(targetdir)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
merge_all_gtfs
merge_all_gtfs(targetdir)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
stopids = list()
for(i in 1:length(datalist)){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
show(length(rawids))
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
rawids
datalist[[i]][["stops"]][["stop_id"]])
datalist[[i]][["stops"]][["stop_id"]]
datalist[[i]][["stops"]]
nrow(datalist[[i]][["stops"]])
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
stopids = list()
for(i in 1:length(datalist)){
if(nrow(datalist[[i]][["stops"]])>0){
rawids = paste0(as.character(datalist[[i]][["stops"]][["stop_id"]]),names(datalist)[i])
show(length(rawids))
# previously used ids? None: ???
#show(length(intersect(names(stopids),rawids)))
intids = as.character(sapply(md5(rawids),hex_to_int))
cleandata[[i]][["stops"]][["stop_id"]] <- intids
stopids[rawids] <- intids
# much more stop times: longer -> use hashset
#cleandata[[i]][["stop_times"]][["stop_id"]] <- sapply(md5(as.character(datalist[[i]][["stop_times"]][["stop_id"]])),hex_to_int)
cleandata[[i]][["stop_times"]][["stop_id"]] <- stopids[paste0(as.character(datalist[[i]][["stop_times"]][["stop_id"]]),names(datalist)[i])]
#show(length(cleandata[[i]][["stops"]][["stop_id"]]))
#show(length(unique(cleandata[[i]][["stops"]][["stop_id"]])))
# idem with stop codes
cleandata[[i]][["stops"]][["stop_code"]] <- as.character(sapply(md5(as.character(datalist[[i]][["stops"]][["stop_code"]])),hex_to_int))
}
}
res = gtfs_merge_force(cleandata, force=T)
res
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
res
merge_all_gtfs(targetdir)
source('~/ComplexSystems/UrbanDynamics/Models/Matsim/Network/functions.R')
setwd(paste0(Sys.getenv('CN_HOME'),'/Models/MesoCoevol'))
library(dplyr)
library(ggplot2)
library(rgdal)
library(rgeos)
source(paste0(Sys.getenv('CN_HOME'),'/Models/Utils/R/plots.R'))
res <- as.tbl(read.csv('MesoCoevol/exploration/2017_08_18_08_05_31_NETWORK_LHS_GRID.csv'))
install.packages("ggplot2")
install.packages("rgdal")
install.packages("rgeos")
setwd(paste0(Sys.getenv('CN_HOME'),'/Models/MesoCoevol'))
library(dplyr)
library(ggplot2)
library(rgdal)
library(rgeos)
source(paste0(Sys.getenv('CN_HOME'),'/Models/Utils/R/plots.R'))
res <- as.tbl(read.csv('MesoCoevol/exploration/2017_08_18_08_05_31_NETWORK_LHS_GRID.csv'))
res$heuristic = floor(res$nwHeuristic)
raw=read.csv(file=paste0(Sys.getenv('CN_HOME'),"/Models/StaticCorrelations/res/res/europe_areasize100_offset50_factor0.5_20160824.csv"),sep=";",header=TRUE)
rows=apply(raw,1,function(r){prod(as.numeric(!is.na(r)))>0})
realres=as.tbl(raw[rows,])
countries = readOGR(paste0(Sys.getenv('CN_HOME'),'/Models/MesoCoevol/analysis/gis'),'countries');country = countries[countries$CNTR_ID=="FR",];datapoints = SpatialPoints(data.frame(realres[,c("lonmin","latmin")]),proj4string = countries@proj4string)
selectedpoints = gContains(country,datapoints,byid = TRUE)
sdata = realres[selectedpoints,]
#rm(raw,realres);gc()
sdata=sdata[apply(sdata,1,function(r){prod(as.numeric(!is.na(r)))>0}),]
cdata=sdata[,c("meanBetweenness","meanPathLength","meanCloseness","networkPerf","diameter")]
summary(cdata)
setwd(paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Models/analysis'))
library(ggplot2)
library(dplyr)
library(GGally)
source(paste0(Sys.getenv('CS_HOME'),'/Organisation/Models/Utils/R/plots.R'))
resdir = paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Results/LHS/');dir.create(resdir,recursive = T)
res <- as.tbl(read.csv(paste0(Sys.getenv('CN_HOME'),'/Models/MesoCoevol/MesoCoevol/exploration/2017_08_18_08_05_31_NETWORK_LHS_GRID.csv')))
res$heuristic = floor(res$nwHeuristic)
res$meanBwCentrality<-as.numeric(as.character(res$meanBwCentrality))
res$meanPathLength<-as.numeric(as.character(res$meanPathLength))
res$nwDiameter<-as.numeric(as.character(res$nwDiameter))
res$meanRelativeSpeed<-as.numeric(as.character(res$meanRelativeSpeed))
res$meanClosenessCentrality<-as.numeric(as.character(res$meanClosenessCentrality))
res$nwLength<-as.numeric(as.character(res$nwLength))
res=res[!is.na(res$meanBwCentrality),]
res=res[res$meanClosenessCentrality<0.2,]
res
summary(res$meanPathLength)
setwd(paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Models/analysis'))
library(ggplot2)
library(dplyr)
library(GGally)
source(paste0(Sys.getenv('CS_HOME'),'/Organisation/Models/Utils/R/plots.R'))
install.packages("GGally")
setwd(paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Models/analysis'))
library(ggplot2)
library(dplyr)
library(GGally)
source(paste0(Sys.getenv('CS_HOME'),'/Organisation/Models/Utils/R/plots.R'))
resdir = paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Results/PSE/');dir.create(resdir,recursive = T)
resdirs = list(
'random' = '../netlogo/pse/20211026_1258_PSE_RANDOM_LOCAL/',
'connection' = '../netlogo/pse/20211103_2144_PSE_CONNECTION_LOCAL/',
'gravity' = '../netlogo/pse/20211028_1101_PSE_GRAVITY_LOCAL/',
'breakdown' = '../netlogo/pse/20211028_0804_PSE_BREAKDOWN_LOCAL/',
'cost' = '20211030_1731_PSE_COST_LOCAL',
'biological' = '../netlogo/pse/20211030_1727_PSE_BIOLOGICAL_LOCAL/'
)
gen<-function(filename){as.integer(sapply(strsplit(sapply(strsplit(filename,"population"),function(s){s[2]}),".csv"),function(s){s[1]}))}
latestgen <- function(dir){
if(is.null(dir)){return(NULL)}
else{return(max(sapply(list.files(dir,pattern=".csv"),gen)))}
}
allres = data.frame()
for(model in names(resdirs)){
currentdir = resdirs[[model]]
currentfiles = paste0(currentdir,'/',list.files(currentdir,pattern = '.csv'))
for(currentfile in currentfiles){
currentres = as.tbl(read.csv(currentfile));currentgen=gen(currentfile)
allres = rbind(allres,cbind(currentres[,indics],model=rep(model,nrow(currentres)),generation=rep(currentgen,nrow(currentres))))
}
}
indics = c("meanBwCentrality","meanClosenessCentrality","meanPathLength","meanRelativeSpeed","nwDiameter","nwLength")
allres = data.frame()
for(model in names(resdirs)){
currentdir = resdirs[[model]]
currentfiles = paste0(currentdir,'/',list.files(currentdir,pattern = '.csv'))
for(currentfile in currentfiles){
currentres = as.tbl(read.csv(currentfile));currentgen=gen(currentfile)
allres = rbind(allres,cbind(currentres[,indics],model=rep(model,nrow(currentres)),generation=rep(currentgen,nrow(currentres))))
}
}
currentdir
resdirs = list(
'random' = '../netlogo/pse/20211026_1258_PSE_RANDOM_LOCAL/',
'connection' = '../netlogo/pse/20211103_2144_PSE_CONNECTION_LOCAL/',
'gravity' = '../netlogo/pse/20211028_1101_PSE_GRAVITY_LOCAL/',
'breakdown' = '../netlogo/pse/20211028_0804_PSE_BREAKDOWN_LOCAL/',
'cost' = '../netlogo/pse/20211030_1731_PSE_COST_LOCAL',
'biological' = '../netlogo/pse/20211030_1727_PSE_BIOLOGICAL_LOCAL/'
)
allres = data.frame()
for(model in names(resdirs)){
currentdir = resdirs[[model]]
currentfiles = paste0(currentdir,'/',list.files(currentdir,pattern = '.csv'))
for(currentfile in currentfiles){
currentres = as.tbl(read.csv(currentfile));currentgen=gen(currentfile)
allres = rbind(allres,cbind(currentres[,indics],model=rep(model,nrow(currentres)),generation=rep(currentgen,nrow(currentres))))
}
}
allres
resdir
summary(cdata)
res = data.frame()
for(model in names(resdirs)){
currentdir = resdirs[[model]]
currentfiles = paste0(currentdir,'/',list.files(currentdir,pattern = '.csv'))
for(currentfile in currentfiles){
currentres = as.tbl(read.csv(currentfile));currentgen=gen(currentfile)
res = rbind(res,cbind(currentres[,indics],model=rep(model,nrow(currentres)),generation=rep(currentgen,nrow(currentres))))
}
}
res=res[!is.na(res$meanBwCentrality),]
res=res[res$meanClosenessCentrality<0.2,]
summary(res)
dim(cdata)
ggsave(plot = ggpairs(res,aes(color=heuristic),columns = indics,
lower = list(continuous = wrap("points", alpha = 0.6,size=0.05,shape='.')),
diag = list(continuous = wrap("densityDiag", alpha = 0.4))
)+stdtheme,filename = paste0(resdir,'scatter_pse_allmodels.png'),width = 40,height=30,units='cm')
res$model
ggsave(plot = ggpairs(res,aes(color=model),columns = indics,
lower = list(continuous = wrap("points", alpha = 0.6,size=0.05,shape='.')),
diag = list(continuous = wrap("densityDiag", alpha = 0.4))
)+stdtheme,filename = paste0(resdir,'scatter_pse_allmodels.png'),width = 40,height=30,units='cm')
indics = c("meanBwCentrality","meanClosenessCentrality","meanPathLength","meanRelativeSpeed","nwDiameter")#,"nwLength")
res = data.frame()
for(model in names(resdirs)){
currentdir = resdirs[[model]]
currentfiles = paste0(currentdir,'/',list.files(currentdir,pattern = '.csv'))
for(currentfile in currentfiles){
currentres = as.tbl(read.csv(currentfile));currentgen=gen(currentfile)
res = rbind(res,cbind(currentres[,indics],model=rep(model,nrow(currentres)),generation=rep(currentgen,nrow(currentres))))
}
}
# some cleaning
res=res[!is.na(res$meanBwCentrality),]
res=res[res$meanClosenessCentrality<0.2,]
ggsave(plot = ggpairs(res,aes(color=model),columns = indics,
lower = list(continuous = wrap("points", alpha = 0.6,size=0.05,shape='.')),
diag = list(continuous = wrap("densityDiag", alpha = 0.4))
)+stdtheme,filename = paste0(resdir,'scatter_pse_allmodels.png'),width = 40,height=30,units='cm')
names(cdata)<-c("meanBwCentrality","meanPathLength","meanClosenessCentrality","meanRelativeSpeed","nwDiameter")
library(hypervolume)
install.packages('hypervolume')
install.packages("reshape2")
library(hypervolume)
library(reshape2)
models = names(resdirs)
set.seed(42)
hvs = list()
for(model in models){
show(model)
start = as.numeric(Sys.time())
sample = res[res$heuristic==model,indics]
sample = sample[sample.int(nrow(sample),size=1000,replace = F),]
hvs[[model]] = hypervolume_gaussian(sample)
show(as.numeric(Sys.time()) - start)
}
sapply(hvs,function(hv){hv@Volume})
sample
hvs = list()
for(model in models){
show(model)
start = as.numeric(Sys.time())
sample = res[res$model==model,indics]
sample = sample[sample.int(nrow(sample),size=1000,replace = F),]
hvs[[model]] = hypervolume_gaussian(sample)
show(as.numeric(Sys.time()) - start)
}
sapply(hvs,function(hv){hv@Volume})
indics
sample
hvs = list()
for(model in models){
show(model)
start = as.numeric(Sys.time())
sample = res[res$model==model,indics]
sample = sample[sample.int(nrow(sample),size=min(1000,nrow(sample)),replace = F),]
hvs[[model]] = hypervolume_gaussian(sample)
show(as.numeric(Sys.time()) - start)
}
sapply(hvs,function(hv){hv@Volume})
overlaps = matrix(data=rep(1,length(models)*length(models)),nrow=length(models))
for(model1 in 1:length(models)){
for (model2 in 1:length(models)){
if(model1!=model2){
show(paste0(models[model1],' / ',models[model2]))
hvset = hypervolume_set(hvs[[models[model1]]],hvs[[models[model2]]],check.memory=FALSE,num.points.max=100000)
overlaps[model1,model2] = hvset@HVList$Intersection@Volume / hvs[[models[model2]]]@Volume
}
}
}
rownames(overlaps)<-models
colnames(overlaps)<-models
diag(overlaps)=NA
save(overlaps,hvs,file=paste0(resdir,'hvs.RData'))
g=ggplot(melt(overlaps),aes(x=Var1,y=Var2,fill=value))
ggsave(g+geom_raster()+xlab('')+ylab('')+scale_fill_continuous(name='Overlap')+stdtheme,
file=paste0(resdir,'pointclouds-overlap.png'),width=30,height=26,units='cm')
cdata=sdata[,c("meanBetweenness","meanPathLength","meanCloseness","networkPerf","diameter")]
write.csv(cdata[,c("meanBwCentrality","meanClosenessCentrality","meanPathLength","meanRelativeSpeed","nwDiameter")],file=paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Models/analysis/data/real.csv'))
names(cdata)<-c("meanBwCentrality","meanPathLength","meanClosenessCentrality","meanRelativeSpeed","nwDiameter")
write.csv(cdata[,c("meanBwCentrality","meanClosenessCentrality","meanPathLength","meanRelativeSpeed","nwDiameter")],file=paste0(Sys.getenv('CS_HOME'),'/NetworkGrowth/Models/analysis/data/real.csv'))
real = read.csv('data/real.csv')
real
summary*res$meanPathLength
summary(res$meanPathLength)
summary(real$meanPathLength)
real$meanPathLength = ((real$meanPathLength - mean(real$meanPathLength))/sd(real$meanPathLength))*sd(res$meanPathLength) + mean(res$meanPathLength)
summary(real$meanPathLength)
real = real[real$meanPathLength<max(res$meanPathLength)]
real = real[real$meanPathLength<max(res$meanPathLength),]
summary(real$meanRelativeSpeed)
real = real[real$meanRelativeSpeed<quantile(real$meanRelativeSpeed,c(0.5)),]
summary(real$meanRelativeSpeed)
real$meanRelativeSpeed = ((real$meanRelativeSpeed - mean(real$meanRelativeSpeed))/sd(real$meanRelativeSpeed))*sd(res$meanRelativeSpeed) + mean(res$meanRelativeSpeed)
summary(real$meanRelativeSpeed)
real = read.csv('data/real.csv')
real$meanPathLength = ((real$meanPathLength - mean(real$meanPathLength))/sd(real$meanPathLength))*sd(res$meanPathLength) + mean(res$meanPathLength)
real = real[real$meanPathLength<max(res$meanPathLength),]
real = real[real$meanRelativeSpeed<quantile(real$meanRelativeSpeed,c(0.8)),]
real$meanRelativeSpeed = ((real$meanRelativeSpeed - mean(real$meanRelativeSpeed))/sd(real$meanRelativeSpeed))*sd(res$meanRelativeSpeed) + mean(res$meanRelativeSpeed)
summary(real$meanRelativeSpeed)
summary(real)
real = real[real$meanClosenessCentrality<0.2,]
real$nwDiameter = ((real$nwDiameter - mean(real$nwDiameter))/sd(real$nwDiameter))*sd(res$nwDiameter) + mean(res$nwDiameter)
summary(real)
real = read.csv('data/real.csv')
real$meanPathLength = ((real$meanPathLength - mean(real$meanPathLength))/sd(real$meanPathLength))*sd(res$meanPathLength) + mean(res$meanPathLength)
real = real[real$meanPathLength<max(res$meanPathLength),]
real = real[real$meanRelativeSpeed<quantile(real$meanRelativeSpeed,c(0.8)),]
real$meanRelativeSpeed = ((real$meanRelativeSpeed - mean(real$meanRelativeSpeed))/sd(real$meanRelativeSpeed))*sd(res$meanRelativeSpeed) + mean(res$meanRelativeSpeed)
summary(real$nwDiameter)
summary(res$nwDiameter)
mean(real$nwDiameter)
real$nwDiameter = ((real$nwDiameter - min(real$nwDiameter))/sd(real$nwDiameter))*sd(res$nwDiameter) + min(res$nwDiameter)
summary(real$nwDiameter)
real=real[real$nwDiameter<2.7,]
real = real[real$meanClosenessCentrality<0.2,]
summary(real)
res = rbind(res,cbind(real[,indics],model=rep('real',nrow(real))))
(real[,indics]
)
dim(res)
res
res = rbind(res,cbind(real,generation=rep(0,nrow(real)),model=rep('real',nrow(real))))
dim(cbind(real,generation=rep(0,nrow(real)),model=rep('real',nrow(real))))
res = rbind(res,cbind(real[,indics],generation=rep(0,nrow(real)),model=rep('real',nrow(real))))
res
ggsave(plot = ggpairs(res,aes(color=model),columns = indics,
lower = list(continuous = wrap("points", alpha = 0.6,size=0.05,shape='.')),
diag = list(continuous = wrap("densityDiag", alpha = 0.4))
)+stdtheme,filename = paste0(resdir,'scatter_pse_withreal.png'),width = 40,height=30,units='cm')
